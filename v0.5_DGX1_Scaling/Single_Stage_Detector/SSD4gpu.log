Clearing caches

:::MLPv0.5.0 ssd 1560273261.065408945 (<string>:1) run_clear_caches
Launching on node dgx1
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX1 -e MULTI_NODE= -e SLURM_JOB_ID=190611101545 -e SLURM_NTASKS_PER_NODE= cont_190611101545 ./run_and_time.sh
Run vars: id 190611101545 gpus 8 mparams 
DGX1
STARTING TIMING RUN AT 2019-06-11 05:14:21 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m torch.distributed.launch --nproc_per_node 4 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 108 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 152 --warmup 300 --nhwc --pad-input
0 Using seed = 1784308032
2 Using seed = 1784308034
3 Using seed = 1784308035
1 Using seed = 1784308033

:::MLPv0.5.0 ssd 1560273268.042195797 (train.py:371) run_start

:::MLPv0.5.0 ssd 1560273268.043093443 (train.py:178) feature_sizes: [38, 19, 10, 5, 3, 1]

:::MLPv0.5.0 ssd 1560273268.043888807 (train.py:180) steps: [8, 16, 32, 64, 100, 300]

:::MLPv0.5.0 ssd 1560273268.044696569 (train.py:183) scales: [21, 45, 99, 153, 207, 261, 315]

:::MLPv0.5.0 ssd 1560273268.045459270 (train.py:185) aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]

:::MLPv0.5.0 ssd 1560273268.109682322 (train.py:188) num_default_boxes: 8732

:::MLPv0.5.0 ssd 1560273268.118575096 (/workspace/single_stage_detector/utils.py:391) num_cropping_iterations: 1

:::MLPv0.5.0 ssd 1560273268.132603168 (/workspace/single_stage_detector/utils.py:510) random_flip_probability: 0.5

:::MLPv0.5.0 ssd 1560273268.135616779 (/workspace/single_stage_detector/utils.py:553) data_normalization_mean: [0.485, 0.456, 0.406]

:::MLPv0.5.0 ssd 1560273268.138521671 (/workspace/single_stage_detector/utils.py:554) data_normalization_std: [0.229, 0.224, 0.225]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...

:::MLPv0.5.0 ssd 1560273268.144341230 (train.py:382) input_size: 300
loading annotations into memory...
Done (t=0.53s)
creating index...
index created!
Done (t=0.57s)
creating index...
index created!
Done (t=0.61s)
creating index...
Done (t=0.61s)
creating index...
index created!
index created!
time_check a: 1560273269.231034279
time_check b: 1560273291.845927715

:::MLPv0.5.0 ssd 1560273295.267584801 (train.py:413) input_order

:::MLPv0.5.0 ssd 1560273295.271008253 (train.py:414) input_batch_size: 152

:::MLPv0.5.0 ssd 1560273296.627611160 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1560273296.628730059 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1560273296.764203310 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()

:::MLPv0.5.0 ssd 1560273297.555910110 (train.py:476) opt_name: "SGD"

:::MLPv0.5.0 ssd 1560273297.556845427 (train.py:477) opt_learning_rate: 0.048

:::MLPv0.5.0 ssd 1560273297.557707548 (train.py:478) opt_momentum: 0.9

:::MLPv0.5.0 ssd 1560273297.558552742 (train.py:480) opt_weight_decay: 0.0005

:::MLPv0.5.0 ssd 1560273297.559364557 (train.py:483) opt_learning_rate_warmup_steps: 300

:::MLPv0.5.0 ssd 1560273298.941516876 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1560273298.942616224 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1560273299.072633266 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
epoch nbatch loss

:::MLPv0.5.0 ssd 1560273302.824169636 (train.py:551) train_loop

:::MLPv0.5.0 ssd 1560273302.825707197 (train.py:553) train_epoch: 0

:::MLPv0.5.0 ssd 1560273302.829557657 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 0, "value": 0.0}
Iteration:      0, Loss function: 22.538, Average Loss: 0.023, avg. samples / sec: 3107.99

:::MLPv0.5.0 ssd 1560273305.061724186 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 1, "value": 0.00016000000000000042}

:::MLPv0.5.0 ssd 1560273305.588098764 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 2, "value": 0.0003199999999999939}

:::MLPv0.5.0 ssd 1560273306.033830404 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 3, "value": 0.0004799999999999943}

:::MLPv0.5.0 ssd 1560273306.549012899 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 4, "value": 0.0006399999999999947}

:::MLPv0.5.0 ssd 1560273307.047451496 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 5, "value": 0.0007999999999999952}

:::MLPv0.5.0 ssd 1560273307.567197084 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 6, "value": 0.0009599999999999956}

:::MLPv0.5.0 ssd 1560273308.043294191 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 7, "value": 0.001119999999999996}

:::MLPv0.5.0 ssd 1560273308.532454491 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 8, "value": 0.0012799999999999964}

:::MLPv0.5.0 ssd 1560273309.027219296 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 9, "value": 0.0014399999999999968}

:::MLPv0.5.0 ssd 1560273309.532396793 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 10, "value": 0.0015999999999999973}

:::MLPv0.5.0 ssd 1560273310.030808926 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 11, "value": 0.0017599999999999977}

:::MLPv0.5.0 ssd 1560273310.512860298 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 12, "value": 0.001919999999999998}

:::MLPv0.5.0 ssd 1560273310.980414867 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 13, "value": 0.0020799999999999985}

:::MLPv0.5.0 ssd 1560273311.457761288 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 14, "value": 0.002239999999999999}

:::MLPv0.5.0 ssd 1560273311.909588337 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 15, "value": 0.0023999999999999994}

:::MLPv0.5.0 ssd 1560273312.378974438 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 16, "value": 0.0025599999999999998}

:::MLPv0.5.0 ssd 1560273312.822007179 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 17, "value": 0.00272}

:::MLPv0.5.0 ssd 1560273313.284046412 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 18, "value": 0.0028800000000000006}

:::MLPv0.5.0 ssd 1560273313.764143944 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 19, "value": 0.003039999999999994}

:::MLPv0.5.0 ssd 1560273314.231643915 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 20, "value": 0.0031999999999999945}
Iteration:     20, Loss function: 20.614, Average Loss: 0.442, avg. samples / sec: 1066.70

:::MLPv0.5.0 ssd 1560273314.688318491 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 21, "value": 0.003359999999999995}

:::MLPv0.5.0 ssd 1560273315.123652697 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 22, "value": 0.0035199999999999954}

:::MLPv0.5.0 ssd 1560273315.602116108 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 23, "value": 0.0036799999999999958}

:::MLPv0.5.0 ssd 1560273316.025165081 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 24, "value": 0.003839999999999996}

:::MLPv0.5.0 ssd 1560273316.466056347 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 25, "value": 0.003999999999999997}

:::MLPv0.5.0 ssd 1560273316.917883158 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 26, "value": 0.004159999999999997}

:::MLPv0.5.0 ssd 1560273317.392116785 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 27, "value": 0.0043199999999999975}

:::MLPv0.5.0 ssd 1560273317.884023905 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 28, "value": 0.004479999999999998}

:::MLPv0.5.0 ssd 1560273318.357576847 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 29, "value": 0.004639999999999998}

:::MLPv0.5.0 ssd 1560273318.815486908 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 30, "value": 0.004799999999999999}

:::MLPv0.5.0 ssd 1560273319.276242971 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 31, "value": 0.004959999999999999}

:::MLPv0.5.0 ssd 1560273319.724565268 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 32, "value": 0.0051199999999999996}

:::MLPv0.5.0 ssd 1560273320.192758083 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 33, "value": 0.00528}

:::MLPv0.5.0 ssd 1560273320.654290915 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 34, "value": 0.00544}

:::MLPv0.5.0 ssd 1560273321.146335602 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 35, "value": 0.005600000000000001}

:::MLPv0.5.0 ssd 1560273321.605653286 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 36, "value": 0.005759999999999994}

:::MLPv0.5.0 ssd 1560273322.075032234 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 37, "value": 0.005919999999999995}

:::MLPv0.5.0 ssd 1560273322.564845562 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 38, "value": 0.006079999999999995}

:::MLPv0.5.0 ssd 1560273322.982510567 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 39, "value": 0.0062399999999999956}

:::MLPv0.5.0 ssd 1560273323.445749760 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 40, "value": 0.006399999999999996}
Iteration:     40, Loss function: 19.184, Average Loss: 0.834, avg. samples / sec: 1319.97

:::MLPv0.5.0 ssd 1560273323.946446657 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 41, "value": 0.006559999999999996}

:::MLPv0.5.0 ssd 1560273324.418644667 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 42, "value": 0.006719999999999997}

:::MLPv0.5.0 ssd 1560273324.800891876 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 43, "value": 0.006879999999999997}

:::MLPv0.5.0 ssd 1560273325.273460150 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 44, "value": 0.007039999999999998}

:::MLPv0.5.0 ssd 1560273325.716260195 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 45, "value": 0.007199999999999998}

:::MLPv0.5.0 ssd 1560273326.158153772 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 46, "value": 0.0073599999999999985}

:::MLPv0.5.0 ssd 1560273326.572006226 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 47, "value": 0.007519999999999999}

:::MLPv0.5.0 ssd 1560273327.027336121 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 48, "value": 0.007679999999999999}

:::MLPv0.5.0 ssd 1560273327.466686249 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 49, "value": 0.00784}

:::MLPv0.5.0 ssd 1560273327.889463186 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 50, "value": 0.008}

:::MLPv0.5.0 ssd 1560273328.361710072 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 51, "value": 0.00816}

:::MLPv0.5.0 ssd 1560273328.815802336 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 52, "value": 0.008320000000000001}

:::MLPv0.5.0 ssd 1560273329.282403469 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 53, "value": 0.008479999999999994}

:::MLPv0.5.0 ssd 1560273329.705107689 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 54, "value": 0.008639999999999995}

:::MLPv0.5.0 ssd 1560273330.135389805 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 55, "value": 0.008799999999999995}

:::MLPv0.5.0 ssd 1560273330.606715441 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 56, "value": 0.008959999999999996}

:::MLPv0.5.0 ssd 1560273331.032829285 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 57, "value": 0.009119999999999996}

:::MLPv0.5.0 ssd 1560273331.458360672 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 58, "value": 0.009279999999999997}

:::MLPv0.5.0 ssd 1560273331.913729429 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 59, "value": 0.009439999999999997}

:::MLPv0.5.0 ssd 1560273332.317766428 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 60, "value": 0.009599999999999997}
Iteration:     60, Loss function: 14.903, Average Loss: 1.125, avg. samples / sec: 1370.58

:::MLPv0.5.0 ssd 1560273332.765628099 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 61, "value": 0.009759999999999998}

:::MLPv0.5.0 ssd 1560273333.189372778 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 62, "value": 0.009919999999999998}

:::MLPv0.5.0 ssd 1560273333.640422583 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 63, "value": 0.010079999999999999}

:::MLPv0.5.0 ssd 1560273334.046976566 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 64, "value": 0.010239999999999999}

:::MLPv0.5.0 ssd 1560273334.522217989 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 65, "value": 0.0104}

:::MLPv0.5.0 ssd 1560273334.996303797 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 66, "value": 0.01056}

:::MLPv0.5.0 ssd 1560273335.432132006 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 67, "value": 0.01072}

:::MLPv0.5.0 ssd 1560273335.868369818 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 68, "value": 0.01088}

:::MLPv0.5.0 ssd 1560273336.292697191 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 69, "value": 0.011040000000000001}

:::MLPv0.5.0 ssd 1560273336.771432638 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 70, "value": 0.011199999999999995}

:::MLPv0.5.0 ssd 1560273337.195359468 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 71, "value": 0.011359999999999995}

:::MLPv0.5.0 ssd 1560273337.611114740 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 72, "value": 0.011519999999999996}

:::MLPv0.5.0 ssd 1560273338.037724972 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 73, "value": 0.011679999999999996}

:::MLPv0.5.0 ssd 1560273338.441744089 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 74, "value": 0.011839999999999996}

:::MLPv0.5.0 ssd 1560273338.853807211 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 75, "value": 0.011999999999999997}

:::MLPv0.5.0 ssd 1560273339.251990318 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 76, "value": 0.012159999999999997}

:::MLPv0.5.0 ssd 1560273339.648027658 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 77, "value": 0.012319999999999998}

:::MLPv0.5.0 ssd 1560273340.078732014 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 78, "value": 0.012479999999999998}

:::MLPv0.5.0 ssd 1560273340.525859118 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 79, "value": 0.012639999999999998}

:::MLPv0.5.0 ssd 1560273340.943531036 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 80, "value": 0.012799999999999999}
Iteration:     80, Loss function: 10.226, Average Loss: 1.346, avg. samples / sec: 1408.89

:::MLPv0.5.0 ssd 1560273341.367915154 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 81, "value": 0.01296}

:::MLPv0.5.0 ssd 1560273341.806307316 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 82, "value": 0.01312}

:::MLPv0.5.0 ssd 1560273342.214614868 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 83, "value": 0.01328}

:::MLPv0.5.0 ssd 1560273342.602378607 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 84, "value": 0.01344}

:::MLPv0.5.0 ssd 1560273343.041813612 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 85, "value": 0.013600000000000001}

:::MLPv0.5.0 ssd 1560273343.433040142 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 86, "value": 0.013760000000000001}

:::MLPv0.5.0 ssd 1560273343.839551210 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 87, "value": 0.013919999999999995}

:::MLPv0.5.0 ssd 1560273344.270971537 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 88, "value": 0.014079999999999995}

:::MLPv0.5.0 ssd 1560273344.679136515 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 89, "value": 0.014239999999999996}

:::MLPv0.5.0 ssd 1560273345.097693920 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 90, "value": 0.014399999999999996}

:::MLPv0.5.0 ssd 1560273345.559098959 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 91, "value": 0.014559999999999997}

:::MLPv0.5.0 ssd 1560273345.951065779 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 92, "value": 0.014719999999999997}

:::MLPv0.5.0 ssd 1560273346.373292685 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 93, "value": 0.014879999999999997}

:::MLPv0.5.0 ssd 1560273346.808058262 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 94, "value": 0.015039999999999998}

:::MLPv0.5.0 ssd 1560273347.203181505 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 95, "value": 0.015199999999999998}

:::MLPv0.5.0 ssd 1560273347.625632524 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 96, "value": 0.015359999999999999}

:::MLPv0.5.0 ssd 1560273348.025496483 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 97, "value": 0.015519999999999999}

:::MLPv0.5.0 ssd 1560273348.424372911 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 98, "value": 0.01568}

:::MLPv0.5.0 ssd 1560273348.838554144 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 99, "value": 0.01584}

:::MLPv0.5.0 ssd 1560273349.253841877 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 100, "value": 0.016}
Iteration:    100, Loss function: 9.352, Average Loss: 1.512, avg. samples / sec: 1463.16

:::MLPv0.5.0 ssd 1560273349.663604498 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 101, "value": 0.01616}

:::MLPv0.5.0 ssd 1560273350.043495655 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 102, "value": 0.01632}

:::MLPv0.5.0 ssd 1560273350.442208529 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 103, "value": 0.01648}

:::MLPv0.5.0 ssd 1560273350.810585737 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 104, "value": 0.016639999999999995}

:::MLPv0.5.0 ssd 1560273351.231186628 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 105, "value": 0.0168}

:::MLPv0.5.0 ssd 1560273351.640268564 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 106, "value": 0.01696}

:::MLPv0.5.0 ssd 1560273352.046095610 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 107, "value": 0.01712}

:::MLPv0.5.0 ssd 1560273352.471592903 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 108, "value": 0.017279999999999997}

:::MLPv0.5.0 ssd 1560273352.860256433 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 109, "value": 0.017439999999999997}

:::MLPv0.5.0 ssd 1560273353.257255077 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 110, "value": 0.017599999999999998}

:::MLPv0.5.0 ssd 1560273353.657972336 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 111, "value": 0.017759999999999998}

:::MLPv0.5.0 ssd 1560273354.046951771 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 112, "value": 0.01792}

:::MLPv0.5.0 ssd 1560273354.454047203 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 113, "value": 0.01808}

:::MLPv0.5.0 ssd 1560273354.885697842 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 114, "value": 0.01824}

:::MLPv0.5.0 ssd 1560273355.298422337 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 115, "value": 0.0184}

:::MLPv0.5.0 ssd 1560273355.698267698 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 116, "value": 0.01856}

:::MLPv0.5.0 ssd 1560273356.085445166 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 117, "value": 0.018719999999999997}

:::MLPv0.5.0 ssd 1560273356.496334791 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 118, "value": 0.018879999999999997}

:::MLPv0.5.0 ssd 1560273356.888747454 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 119, "value": 0.019039999999999998}

:::MLPv0.5.0 ssd 1560273357.251412630 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 120, "value": 0.0192}
Iteration:    120, Loss function: 8.955, Average Loss: 1.664, avg. samples / sec: 1521.25

:::MLPv0.5.0 ssd 1560273357.637637615 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 121, "value": 0.01936}

:::MLPv0.5.0 ssd 1560273357.998597622 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 122, "value": 0.01952}

:::MLPv0.5.0 ssd 1560273358.394447565 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 123, "value": 0.01968}

:::MLPv0.5.0 ssd 1560273358.783286333 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 124, "value": 0.01984}

:::MLPv0.5.0 ssd 1560273359.183079243 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 125, "value": 0.02}

:::MLPv0.5.0 ssd 1560273359.541434288 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 126, "value": 0.020159999999999997}

:::MLPv0.5.0 ssd 1560273359.886401653 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 127, "value": 0.020319999999999998}

:::MLPv0.5.0 ssd 1560273360.252158880 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 128, "value": 0.020479999999999998}

:::MLPv0.5.0 ssd 1560273360.611581326 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 129, "value": 0.02064}

:::MLPv0.5.0 ssd 1560273360.950666904 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 130, "value": 0.0208}

:::MLPv0.5.0 ssd 1560273361.308758974 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 131, "value": 0.02096}

:::MLPv0.5.0 ssd 1560273361.703844547 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 132, "value": 0.02112}

:::MLPv0.5.0 ssd 1560273362.118596077 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 133, "value": 0.02128}

:::MLPv0.5.0 ssd 1560273362.479796886 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 134, "value": 0.021439999999999997}

:::MLPv0.5.0 ssd 1560273362.900661707 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 135, "value": 0.021599999999999998}

:::MLPv0.5.0 ssd 1560273363.302623987 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 136, "value": 0.021759999999999998}

:::MLPv0.5.0 ssd 1560273363.689549685 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 137, "value": 0.02192}

:::MLPv0.5.0 ssd 1560273364.082200766 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 138, "value": 0.02208}

:::MLPv0.5.0 ssd 1560273364.485716343 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 139, "value": 0.02224}

:::MLPv0.5.0 ssd 1560273364.906193972 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 140, "value": 0.0224}
Iteration:    140, Loss function: 9.014, Average Loss: 1.811, avg. samples / sec: 1587.94

:::MLPv0.5.0 ssd 1560273365.339119196 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 141, "value": 0.02256}

:::MLPv0.5.0 ssd 1560273365.707540274 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 142, "value": 0.02272}

:::MLPv0.5.0 ssd 1560273366.052118540 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 143, "value": 0.022879999999999998}

:::MLPv0.5.0 ssd 1560273366.422191620 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 144, "value": 0.023039999999999998}

:::MLPv0.5.0 ssd 1560273366.772843361 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 145, "value": 0.0232}

:::MLPv0.5.0 ssd 1560273367.141171217 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 146, "value": 0.02336}

:::MLPv0.5.0 ssd 1560273367.491407633 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 147, "value": 0.02352}

:::MLPv0.5.0 ssd 1560273367.861836910 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 148, "value": 0.02368}

:::MLPv0.5.0 ssd 1560273368.268002033 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 149, "value": 0.02384}

:::MLPv0.5.0 ssd 1560273368.691830635 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 150, "value": 0.024}

:::MLPv0.5.0 ssd 1560273369.053806305 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 151, "value": 0.024159999999999997}

:::MLPv0.5.0 ssd 1560273369.451407671 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 152, "value": 0.024319999999999998}

:::MLPv0.5.0 ssd 1560273369.791394711 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 153, "value": 0.02448}

:::MLPv0.5.0 ssd 1560273370.158667088 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 154, "value": 0.02464}

:::MLPv0.5.0 ssd 1560273370.500338078 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 155, "value": 0.0248}

:::MLPv0.5.0 ssd 1560273370.895277500 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 156, "value": 0.02496}

:::MLPv0.5.0 ssd 1560273371.266784191 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 157, "value": 0.02512}

:::MLPv0.5.0 ssd 1560273371.627841949 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 158, "value": 0.02528}

:::MLPv0.5.0 ssd 1560273372.000851393 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 159, "value": 0.02544}

:::MLPv0.5.0 ssd 1560273372.372902632 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 160, "value": 0.025599999999999998}
Iteration:    160, Loss function: 8.744, Average Loss: 1.951, avg. samples / sec: 1629.01

:::MLPv0.5.0 ssd 1560273372.743445158 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 161, "value": 0.025759999999999998}

:::MLPv0.5.0 ssd 1560273373.141497612 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 162, "value": 0.02592}

:::MLPv0.5.0 ssd 1560273373.541028023 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 163, "value": 0.02608}

:::MLPv0.5.0 ssd 1560273373.928304195 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 164, "value": 0.02624}

:::MLPv0.5.0 ssd 1560273374.333726645 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 165, "value": 0.0264}

:::MLPv0.5.0 ssd 1560273374.717658043 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 166, "value": 0.02656}

:::MLPv0.5.0 ssd 1560273375.110310793 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 167, "value": 0.02672}

:::MLPv0.5.0 ssd 1560273375.463762999 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 168, "value": 0.026879999999999998}

:::MLPv0.5.0 ssd 1560273375.846177340 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 169, "value": 0.027039999999999998}

:::MLPv0.5.0 ssd 1560273376.217532158 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 170, "value": 0.0272}

:::MLPv0.5.0 ssd 1560273376.582615376 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 171, "value": 0.02736}

:::MLPv0.5.0 ssd 1560273376.957678556 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 172, "value": 0.02752}

:::MLPv0.5.0 ssd 1560273377.382041693 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 173, "value": 0.02768}

:::MLPv0.5.0 ssd 1560273377.751641273 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 174, "value": 0.02784}

:::MLPv0.5.0 ssd 1560273378.104078531 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 175, "value": 0.028}

:::MLPv0.5.0 ssd 1560273378.504196644 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 176, "value": 0.02816}

:::MLPv0.5.0 ssd 1560273378.923466682 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 177, "value": 0.028319999999999998}

:::MLPv0.5.0 ssd 1560273379.330289602 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 178, "value": 0.02848}

:::MLPv0.5.0 ssd 1560273379.774734259 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 179, "value": 0.02864}

:::MLPv0.5.0 ssd 1560273380.136300802 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 180, "value": 0.0288}
Iteration:    180, Loss function: 8.378, Average Loss: 2.082, avg. samples / sec: 1563.71

:::MLPv0.5.0 ssd 1560273380.485990763 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 181, "value": 0.02896}

:::MLPv0.5.0 ssd 1560273380.829302549 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 182, "value": 0.02912}

:::MLPv0.5.0 ssd 1560273381.176455975 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 183, "value": 0.02928}

:::MLPv0.5.0 ssd 1560273381.556791306 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 184, "value": 0.02944}

:::MLPv0.5.0 ssd 1560273381.969260216 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 185, "value": 0.029599999999999998}

:::MLPv0.5.0 ssd 1560273382.299909592 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 186, "value": 0.029759999999999998}

:::MLPv0.5.0 ssd 1560273382.627202511 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 187, "value": 0.02992}

:::MLPv0.5.0 ssd 1560273382.953397274 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 188, "value": 0.03008}

:::MLPv0.5.0 ssd 1560273383.282345295 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 189, "value": 0.03024}

:::MLPv0.5.0 ssd 1560273383.591207027 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 190, "value": 0.0304}

:::MLPv0.5.0 ssd 1560273383.916687965 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 191, "value": 0.03056}

:::MLPv0.5.0 ssd 1560273384.245421171 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 192, "value": 0.03072}

:::MLPv0.5.0 ssd 1560273384.583281755 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 193, "value": 0.03088}

:::MLPv0.5.0 ssd 1560273384.911927462 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 194, "value": 0.031039999999999998}

:::MLPv0.5.0 ssd 1560273385.210493803 (train.py:553) train_epoch: 1

:::MLPv0.5.0 ssd 1560273385.228637934 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 195, "value": 0.0312}

:::MLPv0.5.0 ssd 1560273385.565431833 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 196, "value": 0.03136}

:::MLPv0.5.0 ssd 1560273385.891017199 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 197, "value": 0.03152}

:::MLPv0.5.0 ssd 1560273386.207359314 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 198, "value": 0.03168}

:::MLPv0.5.0 ssd 1560273386.533377886 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 199, "value": 0.03184}

:::MLPv0.5.0 ssd 1560273386.864011526 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 200, "value": 0.032}
Iteration:    200, Loss function: 8.285, Average Loss: 2.210, avg. samples / sec: 1811.00

:::MLPv0.5.0 ssd 1560273387.202132463 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 201, "value": 0.03216}

:::MLPv0.5.0 ssd 1560273387.532680750 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 202, "value": 0.03232}

:::MLPv0.5.0 ssd 1560273387.849939346 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 203, "value": 0.03248}

:::MLPv0.5.0 ssd 1560273388.165035486 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 204, "value": 0.03264}

:::MLPv0.5.0 ssd 1560273388.480859995 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 205, "value": 0.032799999999999996}

:::MLPv0.5.0 ssd 1560273388.807177782 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 206, "value": 0.03296}

:::MLPv0.5.0 ssd 1560273389.121466637 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 207, "value": 0.03312}

:::MLPv0.5.0 ssd 1560273389.447305441 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 208, "value": 0.033280000000000004}

:::MLPv0.5.0 ssd 1560273389.783639193 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 209, "value": 0.03344}

:::MLPv0.5.0 ssd 1560273390.096015930 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 210, "value": 0.0336}

:::MLPv0.5.0 ssd 1560273390.423884869 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 211, "value": 0.03376}

:::MLPv0.5.0 ssd 1560273390.746434927 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 212, "value": 0.03392}

:::MLPv0.5.0 ssd 1560273391.074139118 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 213, "value": 0.03408}

:::MLPv0.5.0 ssd 1560273391.394067764 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 214, "value": 0.03424}

:::MLPv0.5.0 ssd 1560273391.721903801 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 215, "value": 0.0344}

:::MLPv0.5.0 ssd 1560273392.038392067 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 216, "value": 0.03456}

:::MLPv0.5.0 ssd 1560273392.370164394 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 217, "value": 0.03472}

:::MLPv0.5.0 ssd 1560273392.682840824 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 218, "value": 0.03488}

:::MLPv0.5.0 ssd 1560273393.010346889 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 219, "value": 0.03504}

:::MLPv0.5.0 ssd 1560273401.130652905 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 220, "value": 0.0352}
Iteration:    220, Loss function: 8.053, Average Loss: 2.328, avg. samples / sec: 852.09

:::MLPv0.5.0 ssd 1560273401.466087103 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 221, "value": 0.03536}

:::MLPv0.5.0 ssd 1560273401.793580770 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 222, "value": 0.035519999999999996}

:::MLPv0.5.0 ssd 1560273402.122031927 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 223, "value": 0.03568}

:::MLPv0.5.0 ssd 1560273402.447597742 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 224, "value": 0.03584}

:::MLPv0.5.0 ssd 1560273402.775670290 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 225, "value": 0.036000000000000004}

:::MLPv0.5.0 ssd 1560273403.087797403 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 226, "value": 0.03616}

:::MLPv0.5.0 ssd 1560273403.415194988 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 227, "value": 0.03632}

:::MLPv0.5.0 ssd 1560273403.724697828 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 228, "value": 0.03648}

:::MLPv0.5.0 ssd 1560273404.048457146 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 229, "value": 0.03664}

:::MLPv0.5.0 ssd 1560273404.375810623 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 230, "value": 0.0368}

:::MLPv0.5.0 ssd 1560273404.700977802 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 231, "value": 0.03696}

:::MLPv0.5.0 ssd 1560273405.012295008 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 232, "value": 0.03712}

:::MLPv0.5.0 ssd 1560273405.334952116 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 233, "value": 0.03728}

:::MLPv0.5.0 ssd 1560273405.660033226 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 234, "value": 0.03744}

:::MLPv0.5.0 ssd 1560273405.987065792 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 235, "value": 0.0376}

:::MLPv0.5.0 ssd 1560273406.314961433 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 236, "value": 0.03776}

:::MLPv0.5.0 ssd 1560273406.640349388 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 237, "value": 0.03792}

:::MLPv0.5.0 ssd 1560273406.951843739 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 238, "value": 0.03808}

:::MLPv0.5.0 ssd 1560273407.264822960 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 239, "value": 0.038239999999999996}

:::MLPv0.5.0 ssd 1560273407.594215393 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 240, "value": 0.038400000000000004}
Iteration:    240, Loss function: 8.317, Average Loss: 2.445, avg. samples / sec: 1882.67

:::MLPv0.5.0 ssd 1560273407.907060146 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 241, "value": 0.03856}

:::MLPv0.5.0 ssd 1560273408.222708941 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 242, "value": 0.038720000000000004}

:::MLPv0.5.0 ssd 1560273408.548913240 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 243, "value": 0.03888}

:::MLPv0.5.0 ssd 1560273408.874921560 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 244, "value": 0.03904}

:::MLPv0.5.0 ssd 1560273409.184254169 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 245, "value": 0.0392}

:::MLPv0.5.0 ssd 1560273409.519020319 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 246, "value": 0.03936}

:::MLPv0.5.0 ssd 1560273409.851234198 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 247, "value": 0.03952}

:::MLPv0.5.0 ssd 1560273410.179850817 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 248, "value": 0.03968}

:::MLPv0.5.0 ssd 1560273410.492484570 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 249, "value": 0.03984}

:::MLPv0.5.0 ssd 1560273410.805552244 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 250, "value": 0.04}

:::MLPv0.5.0 ssd 1560273411.133950949 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 251, "value": 0.04016}

:::MLPv0.5.0 ssd 1560273411.466874599 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 252, "value": 0.04032}

:::MLPv0.5.0 ssd 1560273411.776330709 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 253, "value": 0.04048}

:::MLPv0.5.0 ssd 1560273412.101099253 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 254, "value": 0.04064}

:::MLPv0.5.0 ssd 1560273412.427250862 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 255, "value": 0.0408}

:::MLPv0.5.0 ssd 1560273412.743960381 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 256, "value": 0.04096}

:::MLPv0.5.0 ssd 1560273413.073262453 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 257, "value": 0.041120000000000004}

:::MLPv0.5.0 ssd 1560273413.407184362 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 258, "value": 0.04128}

:::MLPv0.5.0 ssd 1560273413.737918139 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 259, "value": 0.04144}

:::MLPv0.5.0 ssd 1560273414.061879635 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 260, "value": 0.0416}
Iteration:    260, Loss function: 7.913, Average Loss: 2.554, avg. samples / sec: 1880.48

:::MLPv0.5.0 ssd 1560273414.395643473 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 261, "value": 0.04176}

:::MLPv0.5.0 ssd 1560273414.718952417 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 262, "value": 0.04192}

:::MLPv0.5.0 ssd 1560273415.051975250 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 263, "value": 0.04208}

:::MLPv0.5.0 ssd 1560273415.379823208 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 264, "value": 0.04224}

:::MLPv0.5.0 ssd 1560273415.709685564 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 265, "value": 0.0424}

:::MLPv0.5.0 ssd 1560273416.046509743 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 266, "value": 0.04256}

:::MLPv0.5.0 ssd 1560273416.376664400 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 267, "value": 0.04272}

:::MLPv0.5.0 ssd 1560273416.690285921 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 268, "value": 0.04288}

:::MLPv0.5.0 ssd 1560273416.999654055 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 269, "value": 0.04304}

:::MLPv0.5.0 ssd 1560273417.309338331 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 270, "value": 0.0432}

:::MLPv0.5.0 ssd 1560273417.647096157 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 271, "value": 0.04336}

:::MLPv0.5.0 ssd 1560273417.982640743 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 272, "value": 0.04352}

:::MLPv0.5.0 ssd 1560273418.297996283 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 273, "value": 0.043680000000000004}

:::MLPv0.5.0 ssd 1560273418.610943794 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 274, "value": 0.043840000000000004}

:::MLPv0.5.0 ssd 1560273418.941183805 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 275, "value": 0.044}

:::MLPv0.5.0 ssd 1560273419.255357265 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 276, "value": 0.04416}

:::MLPv0.5.0 ssd 1560273419.584407568 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 277, "value": 0.04432}

:::MLPv0.5.0 ssd 1560273419.900042534 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 278, "value": 0.04448}

:::MLPv0.5.0 ssd 1560273420.227802038 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 279, "value": 0.04464}

:::MLPv0.5.0 ssd 1560273420.542348623 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 280, "value": 0.0448}
Iteration:    280, Loss function: 7.849, Average Loss: 2.659, avg. samples / sec: 1876.47

:::MLPv0.5.0 ssd 1560273420.857994080 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 281, "value": 0.04496}

:::MLPv0.5.0 ssd 1560273421.187069178 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 282, "value": 0.04512}

:::MLPv0.5.0 ssd 1560273421.512454987 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 283, "value": 0.04528}

:::MLPv0.5.0 ssd 1560273421.825768232 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 284, "value": 0.04544}

:::MLPv0.5.0 ssd 1560273422.140604496 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 285, "value": 0.0456}

:::MLPv0.5.0 ssd 1560273422.468720198 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 286, "value": 0.04576}

:::MLPv0.5.0 ssd 1560273422.793442011 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 287, "value": 0.04592}

:::MLPv0.5.0 ssd 1560273423.120131016 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 288, "value": 0.04608}

:::MLPv0.5.0 ssd 1560273423.444571972 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 289, "value": 0.04624}

:::MLPv0.5.0 ssd 1560273423.780846834 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 290, "value": 0.046400000000000004}

:::MLPv0.5.0 ssd 1560273424.101944208 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 291, "value": 0.046560000000000004}

:::MLPv0.5.0 ssd 1560273424.430909395 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 292, "value": 0.04672}

:::MLPv0.5.0 ssd 1560273424.766162395 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 293, "value": 0.04688}

:::MLPv0.5.0 ssd 1560273425.080967665 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 294, "value": 0.04704}

:::MLPv0.5.0 ssd 1560273425.405642986 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 295, "value": 0.0472}

:::MLPv0.5.0 ssd 1560273425.716666460 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 296, "value": 0.04736}

:::MLPv0.5.0 ssd 1560273426.044574022 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 297, "value": 0.04752}

:::MLPv0.5.0 ssd 1560273426.355132103 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 298, "value": 0.04768}

:::MLPv0.5.0 ssd 1560273426.666671038 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 299, "value": 0.04784}
Iteration:    300, Loss function: 7.429, Average Loss: 2.756, avg. samples / sec: 1889.87
Iteration:    320, Loss function: 7.561, Average Loss: 2.849, avg. samples / sec: 1890.94
Iteration:    340, Loss function: 7.234, Average Loss: 2.937, avg. samples / sec: 1885.36
Iteration:    360, Loss function: 7.089, Average Loss: 3.020, avg. samples / sec: 1913.46
Iteration:    380, Loss function: 7.096, Average Loss: 3.103, avg. samples / sec: 1929.05

:::MLPv0.5.0 ssd 1560273456.911269903 (train.py:553) train_epoch: 2
Iteration:    400, Loss function: 6.933, Average Loss: 3.179, avg. samples / sec: 1599.21
Iteration:    420, Loss function: 6.891, Average Loss: 3.251, avg. samples / sec: 1910.05
Iteration:    440, Loss function: 6.511, Average Loss: 3.320, avg. samples / sec: 1897.02
Iteration:    460, Loss function: 6.600, Average Loss: 3.385, avg. samples / sec: 1897.62
Iteration:    480, Loss function: 6.499, Average Loss: 3.446, avg. samples / sec: 1923.18
Iteration:    500, Loss function: 6.229, Average Loss: 3.508, avg. samples / sec: 1924.20
Iteration:    520, Loss function: 6.526, Average Loss: 3.565, avg. samples / sec: 1923.47
Iteration:    540, Loss function: 6.228, Average Loss: 3.620, avg. samples / sec: 1912.50
Iteration:    560, Loss function: 6.177, Average Loss: 3.671, avg. samples / sec: 1909.64
Iteration:    580, Loss function: 5.902, Average Loss: 3.720, avg. samples / sec: 1916.90

:::MLPv0.5.0 ssd 1560273518.592103720 (train.py:553) train_epoch: 3
Iteration:    600, Loss function: 6.053, Average Loss: 3.767, avg. samples / sec: 1929.46
Iteration:    620, Loss function: 6.001, Average Loss: 3.811, avg. samples / sec: 1928.45
Iteration:    640, Loss function: 5.903, Average Loss: 3.856, avg. samples / sec: 1932.80
Iteration:    660, Loss function: 5.802, Average Loss: 3.896, avg. samples / sec: 1928.49
Iteration:    680, Loss function: 5.637, Average Loss: 3.934, avg. samples / sec: 1918.86
Iteration:    700, Loss function: 6.165, Average Loss: 3.973, avg. samples / sec: 1932.37
Iteration:    720, Loss function: 5.685, Average Loss: 4.007, avg. samples / sec: 1935.89
Iteration:    740, Loss function: 5.776, Average Loss: 4.040, avg. samples / sec: 1928.11
Iteration:    760, Loss function: 5.572, Average Loss: 4.070, avg. samples / sec: 1913.73

:::MLPv0.5.0 ssd 1560273580.143959999 (train.py:553) train_epoch: 4
Iteration:    780, Loss function: 5.506, Average Loss: 4.099, avg. samples / sec: 1909.51
Iteration:    800, Loss function: 5.542, Average Loss: 4.130, avg. samples / sec: 1934.38
Iteration:    820, Loss function: 5.418, Average Loss: 4.158, avg. samples / sec: 1932.69
Iteration:    840, Loss function: 5.571, Average Loss: 4.182, avg. samples / sec: 1927.25
Iteration:    860, Loss function: 5.338, Average Loss: 4.207, avg. samples / sec: 1918.92
Iteration:    880, Loss function: 5.465, Average Loss: 4.231, avg. samples / sec: 1930.55
Iteration:    900, Loss function: 5.135, Average Loss: 4.252, avg. samples / sec: 1930.89
Iteration:    920, Loss function: 5.172, Average Loss: 4.273, avg. samples / sec: 1930.03
Iteration:    940, Loss function: 5.258, Average Loss: 4.293, avg. samples / sec: 1931.43
Iteration:    960, Loss function: 5.323, Average Loss: 4.312, avg. samples / sec: 1929.19

:::MLPv0.5.0 ssd 1560273641.280974150 (train.py:553) train_epoch: 5
Iteration:    980, Loss function: 5.382, Average Loss: 4.333, avg. samples / sec: 1939.26
Iteration:   1000, Loss function: 5.214, Average Loss: 4.351, avg. samples / sec: 1944.35
Iteration:   1020, Loss function: 5.279, Average Loss: 4.367, avg. samples / sec: 1934.91
Iteration:   1040, Loss function: 5.006, Average Loss: 4.383, avg. samples / sec: 1928.75
Iteration:   1060, Loss function: 5.012, Average Loss: 4.398, avg. samples / sec: 1924.95
Iteration:   1080, Loss function: 5.014, Average Loss: 4.412, avg. samples / sec: 1944.79
Iteration:   1100, Loss function: 4.943, Average Loss: 4.424, avg. samples / sec: 1926.54
Iteration:   1120, Loss function: 5.014, Average Loss: 4.436, avg. samples / sec: 1928.84
Iteration:   1140, Loss function: 4.904, Average Loss: 4.448, avg. samples / sec: 1939.64
Iteration:   1160, Loss function: 4.979, Average Loss: 4.458, avg. samples / sec: 1919.22

:::MLPv0.5.0 ssd 1560273702.614272356 (train.py:553) train_epoch: 6
Iteration:   1180, Loss function: 4.819, Average Loss: 4.468, avg. samples / sec: 1931.63
Iteration:   1200, Loss function: 5.135, Average Loss: 4.480, avg. samples / sec: 1937.23
Iteration:   1220, Loss function: 5.033, Average Loss: 4.491, avg. samples / sec: 1936.36
Iteration:   1240, Loss function: 4.904, Average Loss: 4.498, avg. samples / sec: 1923.95
Iteration:   1260, Loss function: 4.752, Average Loss: 4.505, avg. samples / sec: 1924.46
Iteration:   1280, Loss function: 4.897, Average Loss: 4.513, avg. samples / sec: 1942.36
Iteration:   1300, Loss function: 5.143, Average Loss: 4.520, avg. samples / sec: 1934.37
Iteration:   1320, Loss function: 4.610, Average Loss: 4.527, avg. samples / sec: 1941.83
Iteration:   1340, Loss function: 5.067, Average Loss: 4.532, avg. samples / sec: 1934.31
Iteration:   1360, Loss function: 4.764, Average Loss: 4.538, avg. samples / sec: 1939.24

:::MLPv0.5.0 ssd 1560273763.582936287 (train.py:553) train_epoch: 7
Iteration:   1380, Loss function: 5.026, Average Loss: 4.543, avg. samples / sec: 1936.85
Iteration:   1400, Loss function: 4.580, Average Loss: 4.548, avg. samples / sec: 1924.88
Iteration:   1420, Loss function: 4.618, Average Loss: 4.552, avg. samples / sec: 1937.41
Iteration:   1440, Loss function: 4.680, Average Loss: 4.556, avg. samples / sec: 1942.63
Iteration:   1460, Loss function: 4.882, Average Loss: 4.560, avg. samples / sec: 1915.97
Iteration:   1480, Loss function: 4.464, Average Loss: 4.564, avg. samples / sec: 1932.24
Iteration:   1500, Loss function: 4.515, Average Loss: 4.566, avg. samples / sec: 1934.61
Iteration:   1520, Loss function: 4.724, Average Loss: 4.568, avg. samples / sec: 1935.88
Iteration:   1540, Loss function: 4.512, Average Loss: 4.569, avg. samples / sec: 1934.54

:::MLPv0.5.0 ssd 1560273824.923926353 (train.py:553) train_epoch: 8
Iteration:   1560, Loss function: 4.763, Average Loss: 4.572, avg. samples / sec: 1934.56
Iteration:   1580, Loss function: 4.710, Average Loss: 4.573, avg. samples / sec: 1938.55
Iteration:   1600, Loss function: 4.760, Average Loss: 4.575, avg. samples / sec: 1943.51
Iteration:   1620, Loss function: 4.544, Average Loss: 4.576, avg. samples / sec: 1939.19
Iteration:   1640, Loss function: 4.947, Average Loss: 4.578, avg. samples / sec: 1936.78
Iteration:   1660, Loss function: 4.950, Average Loss: 4.582, avg. samples / sec: 1937.79
Iteration:   1680, Loss function: 4.591, Average Loss: 4.582, avg. samples / sec: 1932.81
Iteration:   1700, Loss function: 4.563, Average Loss: 4.581, avg. samples / sec: 1935.55
Iteration:   1720, Loss function: 4.438, Average Loss: 4.581, avg. samples / sec: 1946.34
Iteration:   1740, Loss function: 4.542, Average Loss: 4.583, avg. samples / sec: 1928.58

:::MLPv0.5.0 ssd 1560273885.787975073 (train.py:553) train_epoch: 9
Iteration:   1760, Loss function: 4.409, Average Loss: 4.582, avg. samples / sec: 1929.93
Iteration:   1780, Loss function: 4.511, Average Loss: 4.581, avg. samples / sec: 1935.98
Iteration:   1800, Loss function: 4.448, Average Loss: 4.581, avg. samples / sec: 1932.41
Iteration:   1820, Loss function: 4.716, Average Loss: 4.580, avg. samples / sec: 1945.11
Iteration:   1840, Loss function: 4.413, Average Loss: 4.580, avg. samples / sec: 1924.11
Iteration:   1860, Loss function: 4.514, Average Loss: 4.578, avg. samples / sec: 1946.73
Iteration:   1880, Loss function: 4.467, Average Loss: 4.577, avg. samples / sec: 1946.01
Iteration:   1900, Loss function: 4.534, Average Loss: 4.577, avg. samples / sec: 1947.41
Iteration:   1920, Loss function: 4.564, Average Loss: 4.575, avg. samples / sec: 1933.99
Iteration:   1940, Loss function: 4.369, Average Loss: 4.574, avg. samples / sec: 1938.59

:::MLPv0.5.0 ssd 1560273946.959032297 (train.py:553) train_epoch: 10
Iteration:   1960, Loss function: 4.792, Average Loss: 4.572, avg. samples / sec: 1937.65
Iteration:   1980, Loss function: 4.304, Average Loss: 4.572, avg. samples / sec: 1943.96
Iteration:   2000, Loss function: 4.366, Average Loss: 4.569, avg. samples / sec: 1939.86
Iteration:   2020, Loss function: 4.493, Average Loss: 4.566, avg. samples / sec: 1936.19
Iteration:   2040, Loss function: 4.749, Average Loss: 4.565, avg. samples / sec: 1938.40
Iteration:   2060, Loss function: 4.478, Average Loss: 4.562, avg. samples / sec: 1943.37
Iteration:   2080, Loss function: 4.358, Average Loss: 4.559, avg. samples / sec: 1935.46
Iteration:   2100, Loss function: 4.352, Average Loss: 4.557, avg. samples / sec: 1944.28
Iteration:   2120, Loss function: 4.272, Average Loss: 4.555, avg. samples / sec: 1935.26
Iteration:   2140, Loss function: 4.411, Average Loss: 4.553, avg. samples / sec: 1934.04

:::MLPv0.5.0 ssd 1560274008.124109983 (train.py:553) train_epoch: 11
Iteration:   2160, Loss function: 4.469, Average Loss: 4.550, avg. samples / sec: 1937.04
Iteration:   2180, Loss function: 4.410, Average Loss: 4.547, avg. samples / sec: 1924.10
Iteration:   2200, Loss function: 4.166, Average Loss: 4.544, avg. samples / sec: 1934.08
Iteration:   2220, Loss function: 4.234, Average Loss: 4.541, avg. samples / sec: 1926.33
Iteration:   2240, Loss function: 4.563, Average Loss: 4.537, avg. samples / sec: 1947.11
Iteration:   2260, Loss function: 4.500, Average Loss: 4.534, avg. samples / sec: 1941.31
Iteration:   2280, Loss function: 4.272, Average Loss: 4.530, avg. samples / sec: 1944.80
Iteration:   2300, Loss function: 4.239, Average Loss: 4.525, avg. samples / sec: 1940.33
Iteration:   2320, Loss function: 4.499, Average Loss: 4.522, avg. samples / sec: 1932.43

:::MLPv0.5.0 ssd 1560274069.038965225 (train.py:553) train_epoch: 12
Iteration:   2340, Loss function: 4.289, Average Loss: 4.519, avg. samples / sec: 1938.38
Iteration:   2360, Loss function: 4.336, Average Loss: 4.516, avg. samples / sec: 1943.08
Iteration:   2380, Loss function: 4.480, Average Loss: 4.513, avg. samples / sec: 1945.79
Iteration:   2400, Loss function: 4.260, Average Loss: 4.509, avg. samples / sec: 1929.94
Iteration:   2420, Loss function: 4.204, Average Loss: 4.506, avg. samples / sec: 1942.69
Iteration:   2440, Loss function: 4.278, Average Loss: 4.504, avg. samples / sec: 1944.34
Iteration:   2460, Loss function: 4.172, Average Loss: 4.500, avg. samples / sec: 1945.87
Iteration:   2480, Loss function: 4.337, Average Loss: 4.496, avg. samples / sec: 1941.67
Iteration:   2500, Loss function: 4.309, Average Loss: 4.491, avg. samples / sec: 1947.04
Iteration:   2520, Loss function: 4.391, Average Loss: 4.488, avg. samples / sec: 1938.06

:::MLPv0.5.0 ssd 1560274130.087026119 (train.py:553) train_epoch: 13
Iteration:   2540, Loss function: 4.248, Average Loss: 4.484, avg. samples / sec: 1944.49
Iteration:   2560, Loss function: 4.345, Average Loss: 4.479, avg. samples / sec: 1939.89
Iteration:   2580, Loss function: 4.234, Average Loss: 4.475, avg. samples / sec: 1940.42
Iteration:   2600, Loss function: 4.214, Average Loss: 4.473, avg. samples / sec: 1940.87
Iteration:   2620, Loss function: 4.268, Average Loss: 4.469, avg. samples / sec: 1935.17
Iteration:   2640, Loss function: 4.186, Average Loss: 4.465, avg. samples / sec: 1937.16
Iteration:   2660, Loss function: 4.374, Average Loss: 4.461, avg. samples / sec: 1937.15
Iteration:   2680, Loss function: 4.432, Average Loss: 4.459, avg. samples / sec: 1939.73
Iteration:   2700, Loss function: 4.120, Average Loss: 4.454, avg. samples / sec: 1942.36
Iteration:   2720, Loss function: 4.124, Average Loss: 4.450, avg. samples / sec: 1931.07

:::MLPv0.5.0 ssd 1560274190.927395582 (train.py:553) train_epoch: 14
Iteration:   2740, Loss function: 4.211, Average Loss: 4.447, avg. samples / sec: 1946.02
Iteration:   2760, Loss function: 4.245, Average Loss: 4.443, avg. samples / sec: 1943.20
Iteration:   2780, Loss function: 4.220, Average Loss: 4.441, avg. samples / sec: 1941.79
Iteration:   2800, Loss function: 4.373, Average Loss: 4.437, avg. samples / sec: 1938.05
Iteration:   2820, Loss function: 4.101, Average Loss: 4.432, avg. samples / sec: 1940.61
Iteration:   2840, Loss function: 3.980, Average Loss: 4.429, avg. samples / sec: 1944.82
Iteration:   2860, Loss function: 4.448, Average Loss: 4.426, avg. samples / sec: 1946.95
Iteration:   2880, Loss function: 4.240, Average Loss: 4.421, avg. samples / sec: 1945.43
Iteration:   2900, Loss function: 4.087, Average Loss: 4.416, avg. samples / sec: 1942.76

:::MLPv0.5.0 ssd 1560274251.931426287 (train.py:553) train_epoch: 15
Iteration:   2920, Loss function: 4.165, Average Loss: 4.413, avg. samples / sec: 1945.05
Iteration:   2940, Loss function: 4.134, Average Loss: 4.408, avg. samples / sec: 1945.10
Iteration:   2960, Loss function: 4.096, Average Loss: 4.404, avg. samples / sec: 1940.43
Iteration:   2980, Loss function: 3.827, Average Loss: 4.399, avg. samples / sec: 1940.16
Iteration:   3000, Loss function: 4.107, Average Loss: 4.396, avg. samples / sec: 1936.73
Iteration:   3020, Loss function: 4.290, Average Loss: 4.391, avg. samples / sec: 1935.34
Iteration:   3040, Loss function: 4.181, Average Loss: 4.386, avg. samples / sec: 1940.57
Iteration:   3060, Loss function: 4.052, Average Loss: 4.381, avg. samples / sec: 1936.32
Iteration:   3080, Loss function: 4.007, Average Loss: 4.377, avg. samples / sec: 1945.71
Iteration:   3100, Loss function: 4.053, Average Loss: 4.371, avg. samples / sec: 1945.13

:::MLPv0.5.0 ssd 1560274312.716022491 (train.py:553) train_epoch: 16
Iteration:   3120, Loss function: 4.011, Average Loss: 4.368, avg. samples / sec: 1935.58
Iteration:   3140, Loss function: 4.228, Average Loss: 4.363, avg. samples / sec: 1938.33
Iteration:   3160, Loss function: 3.984, Average Loss: 4.360, avg. samples / sec: 1939.67
Iteration:   3180, Loss function: 3.864, Average Loss: 4.355, avg. samples / sec: 1940.03
Iteration:   3200, Loss function: 4.326, Average Loss: 4.351, avg. samples / sec: 1939.94
Iteration:   3220, Loss function: 4.029, Average Loss: 4.349, avg. samples / sec: 1939.97
Iteration:   3240, Loss function: 3.994, Average Loss: 4.344, avg. samples / sec: 1946.09
Iteration:   3260, Loss function: 4.054, Average Loss: 4.339, avg. samples / sec: 1937.17
Iteration:   3280, Loss function: 4.262, Average Loss: 4.335, avg. samples / sec: 1938.46
Iteration:   3300, Loss function: 3.932, Average Loss: 4.333, avg. samples / sec: 1939.32

:::MLPv0.5.0 ssd 1560274373.841147900 (train.py:553) train_epoch: 17
Iteration:   3320, Loss function: 4.288, Average Loss: 4.329, avg. samples / sec: 1939.23
Iteration:   3340, Loss function: 4.120, Average Loss: 4.325, avg. samples / sec: 1942.14
Iteration:   3360, Loss function: 4.167, Average Loss: 4.320, avg. samples / sec: 1932.89
Iteration:   3380, Loss function: 4.397, Average Loss: 4.318, avg. samples / sec: 1937.32
Iteration:   3400, Loss function: 4.290, Average Loss: 4.312, avg. samples / sec: 1933.61
Iteration:   3420, Loss function: 4.260, Average Loss: 4.310, avg. samples / sec: 1938.62
Iteration:   3440, Loss function: 4.230, Average Loss: 4.307, avg. samples / sec: 1936.07
Iteration:   3460, Loss function: 3.990, Average Loss: 4.303, avg. samples / sec: 1940.62
Iteration:   3480, Loss function: 4.112, Average Loss: 4.298, avg. samples / sec: 1934.19
Iteration:   3500, Loss function: 3.895, Average Loss: 4.294, avg. samples / sec: 1938.60

:::MLPv0.5.0 ssd 1560274434.734624624 (train.py:553) train_epoch: 18
Iteration:   3520, Loss function: 4.251, Average Loss: 4.290, avg. samples / sec: 1938.34
Iteration:   3540, Loss function: 3.961, Average Loss: 4.286, avg. samples / sec: 1950.05
Iteration:   3560, Loss function: 4.192, Average Loss: 4.282, avg. samples / sec: 1949.91
Iteration:   3580, Loss function: 4.117, Average Loss: 4.278, avg. samples / sec: 1948.36
Iteration:   3600, Loss function: 4.078, Average Loss: 4.273, avg. samples / sec: 1945.83
Iteration:   3620, Loss function: 4.052, Average Loss: 4.269, avg. samples / sec: 1942.64
Iteration:   3640, Loss function: 4.020, Average Loss: 4.266, avg. samples / sec: 1939.26
Iteration:   3660, Loss function: 4.224, Average Loss: 4.262, avg. samples / sec: 1941.54
Iteration:   3680, Loss function: 4.100, Average Loss: 4.258, avg. samples / sec: 1944.94

:::MLPv0.5.0 ssd 1560274495.715722322 (train.py:553) train_epoch: 19
Iteration:   3700, Loss function: 4.076, Average Loss: 4.255, avg. samples / sec: 1940.72
Iteration:   3720, Loss function: 4.118, Average Loss: 4.251, avg. samples / sec: 1943.59
Iteration:   3740, Loss function: 3.843, Average Loss: 4.247, avg. samples / sec: 1938.01
Iteration:   3760, Loss function: 3.673, Average Loss: 4.244, avg. samples / sec: 1937.22
Iteration:   3780, Loss function: 4.069, Average Loss: 4.240, avg. samples / sec: 1942.84
Iteration:   3800, Loss function: 4.211, Average Loss: 4.238, avg. samples / sec: 1949.95
Iteration:   3820, Loss function: 4.140, Average Loss: 4.234, avg. samples / sec: 1943.83
Iteration:   3840, Loss function: 4.049, Average Loss: 4.231, avg. samples / sec: 1946.46
Iteration:   3860, Loss function: 4.070, Average Loss: 4.226, avg. samples / sec: 1936.78
Iteration:   3880, Loss function: 4.210, Average Loss: 4.222, avg. samples / sec: 1943.79

:::MLPv0.5.0 ssd 1560274556.758218527 (train.py:553) train_epoch: 20
Iteration:   3900, Loss function: 4.046, Average Loss: 4.219, avg. samples / sec: 1939.03
Iteration:   3920, Loss function: 4.216, Average Loss: 4.216, avg. samples / sec: 1941.60
Iteration:   3940, Loss function: 4.008, Average Loss: 4.212, avg. samples / sec: 1939.43
Iteration:   3960, Loss function: 3.952, Average Loss: 4.208, avg. samples / sec: 1948.34
Iteration:   3980, Loss function: 4.085, Average Loss: 4.205, avg. samples / sec: 1947.49
Iteration:   4000, Loss function: 3.954, Average Loss: 4.202, avg. samples / sec: 1945.16
Iteration:   4020, Loss function: 3.697, Average Loss: 4.198, avg. samples / sec: 1941.46
Iteration:   4040, Loss function: 4.090, Average Loss: 4.194, avg. samples / sec: 1946.80
Iteration:   4060, Loss function: 4.234, Average Loss: 4.190, avg. samples / sec: 1943.49
Iteration:   4080, Loss function: 3.913, Average Loss: 4.188, avg. samples / sec: 1930.76

:::MLPv0.5.0 ssd 1560274617.482178211 (train.py:553) train_epoch: 21
Iteration:   4100, Loss function: 4.056, Average Loss: 4.184, avg. samples / sec: 1943.43
Iteration:   4120, Loss function: 3.803, Average Loss: 4.180, avg. samples / sec: 1944.94
Iteration:   4140, Loss function: 3.970, Average Loss: 4.176, avg. samples / sec: 1948.25
Iteration:   4160, Loss function: 3.873, Average Loss: 4.173, avg. samples / sec: 1938.21
Iteration:   4180, Loss function: 3.996, Average Loss: 4.171, avg. samples / sec: 1942.65
Iteration:   4200, Loss function: 4.245, Average Loss: 4.167, avg. samples / sec: 1949.31
Iteration:   4220, Loss function: 4.013, Average Loss: 4.164, avg. samples / sec: 1939.00
Iteration:   4240, Loss function: 4.003, Average Loss: 4.161, avg. samples / sec: 1945.51
Iteration:   4260, Loss function: 4.174, Average Loss: 4.158, avg. samples / sec: 1942.56
Iteration:   4280, Loss function: 3.957, Average Loss: 4.156, avg. samples / sec: 1932.85

:::MLPv0.5.0 ssd 1560274678.512720823 (train.py:553) train_epoch: 22
Iteration:   4300, Loss function: 4.055, Average Loss: 4.153, avg. samples / sec: 1934.60
Iteration:   4320, Loss function: 3.738, Average Loss: 4.151, avg. samples / sec: 1942.93
Iteration:   4340, Loss function: 4.037, Average Loss: 4.147, avg. samples / sec: 1945.31
Iteration:   4360, Loss function: 3.942, Average Loss: 4.143, avg. samples / sec: 1946.32
Iteration:   4380, Loss function: 4.158, Average Loss: 4.140, avg. samples / sec: 1940.78
Iteration:   4400, Loss function: 4.037, Average Loss: 4.137, avg. samples / sec: 1943.74
Iteration:   4420, Loss function: 4.176, Average Loss: 4.135, avg. samples / sec: 1935.28
Iteration:   4440, Loss function: 3.995, Average Loss: 4.132, avg. samples / sec: 1942.43
Iteration:   4460, Loss function: 3.955, Average Loss: 4.128, avg. samples / sec: 1933.46

:::MLPv0.5.0 ssd 1560274739.298426628 (train.py:553) train_epoch: 23
Iteration:   4480, Loss function: 3.893, Average Loss: 4.125, avg. samples / sec: 1938.13
Iteration:   4500, Loss function: 3.818, Average Loss: 4.122, avg. samples / sec: 1943.22
Iteration:   4520, Loss function: 4.042, Average Loss: 4.119, avg. samples / sec: 1939.13
Iteration:   4540, Loss function: 3.871, Average Loss: 4.115, avg. samples / sec: 1939.83
Iteration:   4560, Loss function: 3.994, Average Loss: 4.111, avg. samples / sec: 1940.28
Iteration:   4580, Loss function: 4.080, Average Loss: 4.109, avg. samples / sec: 1937.35
Iteration:   4600, Loss function: 3.714, Average Loss: 4.105, avg. samples / sec: 1948.65
Iteration:   4620, Loss function: 3.856, Average Loss: 4.103, avg. samples / sec: 1946.54
Iteration:   4640, Loss function: 3.900, Average Loss: 4.098, avg. samples / sec: 1945.76
Iteration:   4660, Loss function: 3.911, Average Loss: 4.095, avg. samples / sec: 1937.88

:::MLPv0.5.0 ssd 1560274800.340249062 (train.py:553) train_epoch: 24
Iteration:   4680, Loss function: 4.210, Average Loss: 4.092, avg. samples / sec: 1948.51
Iteration:   4700, Loss function: 4.096, Average Loss: 4.090, avg. samples / sec: 1942.20
Iteration:   4720, Loss function: 3.981, Average Loss: 4.086, avg. samples / sec: 1932.65
Iteration:   4740, Loss function: 3.981, Average Loss: 4.083, avg. samples / sec: 1945.01
Iteration:   4760, Loss function: 4.016, Average Loss: 4.081, avg. samples / sec: 1942.65
Iteration:   4780, Loss function: 4.330, Average Loss: 4.080, avg. samples / sec: 1944.48
Iteration:   4800, Loss function: 3.770, Average Loss: 4.075, avg. samples / sec: 1945.82
Iteration:   4820, Loss function: 3.886, Average Loss: 4.073, avg. samples / sec: 1946.54
Iteration:   4840, Loss function: 4.076, Average Loss: 4.069, avg. samples / sec: 1944.80
Iteration:   4860, Loss function: 3.886, Average Loss: 4.067, avg. samples / sec: 1945.96

:::MLPv0.5.0 ssd 1560274861.036551476 (train.py:553) train_epoch: 25
Iteration:   4880, Loss function: 4.101, Average Loss: 4.065, avg. samples / sec: 1938.42
Iteration:   4900, Loss function: 3.602, Average Loss: 4.061, avg. samples / sec: 1945.60
Iteration:   4920, Loss function: 3.812, Average Loss: 4.059, avg. samples / sec: 1934.98
Iteration:   4940, Loss function: 3.861, Average Loss: 4.058, avg. samples / sec: 1940.67
Iteration:   4960, Loss function: 3.789, Average Loss: 4.054, avg. samples / sec: 1945.92
Iteration:   4980, Loss function: 3.870, Average Loss: 4.051, avg. samples / sec: 1945.00
Iteration:   5000, Loss function: 4.012, Average Loss: 4.049, avg. samples / sec: 1938.66
Iteration:   5020, Loss function: 3.998, Average Loss: 4.046, avg. samples / sec: 1946.47
Iteration:   5040, Loss function: 4.015, Average Loss: 4.044, avg. samples / sec: 1936.21

:::MLPv0.5.0 ssd 1560274922.083270550 (train.py:553) train_epoch: 26
Iteration:   5060, Loss function: 3.691, Average Loss: 4.041, avg. samples / sec: 1948.18
Iteration:   5080, Loss function: 4.134, Average Loss: 4.039, avg. samples / sec: 1941.64
Iteration:   5100, Loss function: 3.691, Average Loss: 4.038, avg. samples / sec: 1945.29
Iteration:   5120, Loss function: 3.656, Average Loss: 4.035, avg. samples / sec: 1939.93
Iteration:   5140, Loss function: 3.928, Average Loss: 4.032, avg. samples / sec: 1940.03
Iteration:   5160, Loss function: 4.063, Average Loss: 4.031, avg. samples / sec: 1943.83
Iteration:   5180, Loss function: 3.915, Average Loss: 4.029, avg. samples / sec: 1944.91
Iteration:   5200, Loss function: 3.836, Average Loss: 4.027, avg. samples / sec: 1949.52
Iteration:   5220, Loss function: 3.922, Average Loss: 4.023, avg. samples / sec: 1946.09
Iteration:   5240, Loss function: 4.187, Average Loss: 4.022, avg. samples / sec: 1951.24

:::MLPv0.5.0 ssd 1560274982.725111246 (train.py:553) train_epoch: 27
Iteration:   5260, Loss function: 3.983, Average Loss: 4.020, avg. samples / sec: 1944.83
Iteration:   5280, Loss function: 3.912, Average Loss: 4.017, avg. samples / sec: 1939.88
Iteration:   5300, Loss function: 3.570, Average Loss: 4.014, avg. samples / sec: 1948.52
Iteration:   5320, Loss function: 3.981, Average Loss: 4.012, avg. samples / sec: 1933.33
Iteration:   5340, Loss function: 3.735, Average Loss: 4.009, avg. samples / sec: 1947.32
Iteration:   5360, Loss function: 3.852, Average Loss: 4.007, avg. samples / sec: 1939.18
Iteration:   5380, Loss function: 3.874, Average Loss: 4.004, avg. samples / sec: 1949.21
Iteration:   5400, Loss function: 3.904, Average Loss: 4.002, avg. samples / sec: 1947.47
Iteration:   5420, Loss function: 3.853, Average Loss: 3.996, avg. samples / sec: 1940.06
Iteration:   5440, Loss function: 3.964, Average Loss: 3.995, avg. samples / sec: 1945.70

:::MLPv0.5.0 ssd 1560275043.740722895 (train.py:553) train_epoch: 28
Iteration:   5460, Loss function: 4.154, Average Loss: 3.994, avg. samples / sec: 1949.40
Iteration:   5480, Loss function: 3.812, Average Loss: 3.992, avg. samples / sec: 1945.64
Iteration:   5500, Loss function: 3.815, Average Loss: 3.989, avg. samples / sec: 1945.23
Iteration:   5520, Loss function: 3.772, Average Loss: 3.987, avg. samples / sec: 1946.47
Iteration:   5540, Loss function: 3.717, Average Loss: 3.985, avg. samples / sec: 1944.29
Iteration:   5560, Loss function: 3.700, Average Loss: 3.983, avg. samples / sec: 1942.44
Iteration:   5580, Loss function: 3.774, Average Loss: 3.979, avg. samples / sec: 1946.37
Iteration:   5600, Loss function: 3.694, Average Loss: 3.977, avg. samples / sec: 1946.70
Iteration:   5620, Loss function: 3.501, Average Loss: 3.974, avg. samples / sec: 1950.25
Iteration:   5640, Loss function: 3.877, Average Loss: 3.972, avg. samples / sec: 1947.56

:::MLPv0.5.0 ssd 1560275104.344997406 (train.py:553) train_epoch: 29
Iteration:   5660, Loss function: 3.829, Average Loss: 3.971, avg. samples / sec: 1941.29
Iteration:   5680, Loss function: 3.876, Average Loss: 3.968, avg. samples / sec: 1942.77
Iteration:   5700, Loss function: 3.680, Average Loss: 3.966, avg. samples / sec: 1941.52
Iteration:   5720, Loss function: 3.767, Average Loss: 3.964, avg. samples / sec: 1944.97
Iteration:   5740, Loss function: 3.820, Average Loss: 3.962, avg. samples / sec: 1944.82
Iteration:   5760, Loss function: 4.060, Average Loss: 3.960, avg. samples / sec: 1938.35
Iteration:   5780, Loss function: 3.846, Average Loss: 3.958, avg. samples / sec: 1942.49
Iteration:   5800, Loss function: 3.615, Average Loss: 3.957, avg. samples / sec: 1946.17
Iteration:   5820, Loss function: 4.131, Average Loss: 3.955, avg. samples / sec: 1947.84

:::MLPv0.5.0 ssd 1560275165.355838776 (train.py:553) train_epoch: 30
Iteration:   5840, Loss function: 3.867, Average Loss: 3.953, avg. samples / sec: 1940.86
Iteration:   5860, Loss function: 4.099, Average Loss: 3.952, avg. samples / sec: 1945.16
Iteration:   5880, Loss function: 3.907, Average Loss: 3.950, avg. samples / sec: 1946.71
Iteration:   5900, Loss function: 3.927, Average Loss: 3.948, avg. samples / sec: 1946.05
Iteration:   5920, Loss function: 4.128, Average Loss: 3.945, avg. samples / sec: 1947.09
Iteration:   5940, Loss function: 3.730, Average Loss: 3.942, avg. samples / sec: 1936.45
Iteration:   5960, Loss function: 3.830, Average Loss: 3.941, avg. samples / sec: 1942.58
Iteration:   5980, Loss function: 3.900, Average Loss: 3.939, avg. samples / sec: 1942.85
Iteration:   6000, Loss function: 3.601, Average Loss: 3.937, avg. samples / sec: 1940.84
Iteration:   6020, Loss function: 3.924, Average Loss: 3.935, avg. samples / sec: 1946.19

:::MLPv0.5.0 ssd 1560275226.348178148 (train.py:553) train_epoch: 31
Iteration:   6040, Loss function: 4.028, Average Loss: 3.934, avg. samples / sec: 1947.80
Iteration:   6060, Loss function: 3.869, Average Loss: 3.932, avg. samples / sec: 1945.62
Iteration:   6080, Loss function: 3.758, Average Loss: 3.929, avg. samples / sec: 1940.99
Iteration:   6100, Loss function: 3.924, Average Loss: 3.927, avg. samples / sec: 1942.60
Iteration:   6120, Loss function: 3.860, Average Loss: 3.925, avg. samples / sec: 1940.56
Iteration:   6140, Loss function: 3.900, Average Loss: 3.924, avg. samples / sec: 1944.95
Iteration:   6160, Loss function: 3.857, Average Loss: 3.921, avg. samples / sec: 1942.31
Iteration:   6180, Loss function: 3.796, Average Loss: 3.919, avg. samples / sec: 1949.71
Iteration:   6200, Loss function: 3.520, Average Loss: 3.916, avg. samples / sec: 1938.63
Iteration:   6220, Loss function: 3.799, Average Loss: 3.915, avg. samples / sec: 1943.86

:::MLPv0.5.0 ssd 1560275287.058044434 (train.py:553) train_epoch: 32
Iteration:   6240, Loss function: 3.871, Average Loss: 3.912, avg. samples / sec: 1938.99
Iteration:   6260, Loss function: 3.716, Average Loss: 3.910, avg. samples / sec: 1944.81
Iteration:   6280, Loss function: 3.543, Average Loss: 3.908, avg. samples / sec: 1938.34
Iteration:   6300, Loss function: 3.595, Average Loss: 3.905, avg. samples / sec: 1946.80





:::MLPv0.5.0 ssd 1560275315.202950001 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1560275315.203588247 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1560275315.204099178 (train.py:220) eval_start: 32
Parsing batch: 0/12Parsing batch: 0/12Parsing batch: 0/12Parsing batch: 0/12Parsing batch: 1/12Parsing batch: 1/12Parsing batch: 1/12Parsing batch: 1/12Parsing batch: 2/12Parsing batch: 2/12Parsing batch: 2/12Parsing batch: 2/12Parsing batch: 3/12Parsing batch: 3/12Parsing batch: 3/12Parsing batch: 3/12Parsing batch: 4/12Parsing batch: 4/12Parsing batch: 4/12Parsing batch: 4/12Parsing batch: 5/12Parsing batch: 5/12Parsing batch: 5/12Parsing batch: 5/12Parsing batch: 6/12Parsing batch: 6/12Parsing batch: 6/12Parsing batch: 7/12Parsing batch: 6/12Parsing batch: 7/12Parsing batch: 7/12No object detected in idx: 13
Parsing batch: 8/12Parsing batch: 8/12Parsing batch: 8/12Parsing batch: 7/12Parsing batch: 9/12Parsing batch: 9/12Parsing batch: 9/12Parsing batch: 8/12Parsing batch: 10/12Parsing batch: 10/12Parsing batch: 10/12Parsing batch: 9/12Parsing batch: 11/12Parsing batch: 11/12Parsing batch: 11/12Parsing batch: 10/12Parsing batch: 11/12Predicting Ended, total time: 79.28 s
Loading and preparing results...
Converting ndarray to lists...
(347639, 7)
0/347639
Loading and preparing results...
Converting ndarray to lists...
(347639, 7)
0/347639
Loading and preparing results...
Converting ndarray to lists...
(347639, 7)
0/347639
Loading and preparing results...
Converting ndarray to lists...
(347639, 7)
0/347639
DONE (t=2.16s)
creating index...
index created!
DONE (t=2.32s)
creating index...
DONE (t=2.33s)
creating index...
DONE (t=2.39s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=3.94s).
Accumulating evaluation results...
DONE (t=1.41s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.155
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.293
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.042
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.167
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.250
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.263
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.072
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.285
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.411
Current AP: 0.15464 AP goal: 0.21200

:::MLPv0.5.0 ssd 1560275402.535058022 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1560275402.535856247 (train.py:333) eval_accuracy: {"epoch": 32, "value": 0.1546392316069332}

:::MLPv0.5.0 ssd 1560275402.536381960 (train.py:336) eval_iteration_accuracy: {"epoch": 32, "value": 0.1546392316069332}

:::MLPv0.5.0 ssd 1560275402.536890984 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1560275402.537405968 (train.py:338) eval_stop: 32
Iteration:   6320, Loss function: 3.992, Average Loss: 3.905, avg. samples / sec: 129.21
Iteration:   6340, Loss function: 3.694, Average Loss: 3.903, avg. samples / sec: 1969.64
Iteration:   6360, Loss function: 3.702, Average Loss: 3.900, avg. samples / sec: 1962.99
Iteration:   6380, Loss function: 3.800, Average Loss: 3.898, avg. samples / sec: 1956.28
Iteration:   6400, Loss function: 3.682, Average Loss: 3.897, avg. samples / sec: 1956.57
Iteration:   6420, Loss function: 3.865, Average Loss: 3.895, avg. samples / sec: 1954.40

:::MLPv0.5.0 ssd 1560275435.646926165 (train.py:553) train_epoch: 33
Iteration:   6440, Loss function: 3.782, Average Loss: 3.894, avg. samples / sec: 1949.14
Iteration:   6460, Loss function: 3.888, Average Loss: 3.893, avg. samples / sec: 1952.66
Iteration:   6480, Loss function: 3.750, Average Loss: 3.891, avg. samples / sec: 1952.45
Iteration:   6500, Loss function: 3.973, Average Loss: 3.890, avg. samples / sec: 1951.65
Iteration:   6520, Loss function: 3.623, Average Loss: 3.888, avg. samples / sec: 1950.44
Iteration:   6540, Loss function: 3.602, Average Loss: 3.887, avg. samples / sec: 1950.38
Iteration:   6560, Loss function: 4.090, Average Loss: 3.885, avg. samples / sec: 1945.53
Iteration:   6580, Loss function: 3.744, Average Loss: 3.884, avg. samples / sec: 1947.00
Iteration:   6600, Loss function: 3.804, Average Loss: 3.882, avg. samples / sec: 1949.41

:::MLPv0.5.0 ssd 1560275496.162666559 (train.py:553) train_epoch: 34
Iteration:   6620, Loss function: 3.787, Average Loss: 3.880, avg. samples / sec: 1943.32
Iteration:   6640, Loss function: 4.134, Average Loss: 3.879, avg. samples / sec: 1950.64
Iteration:   6660, Loss function: 3.746, Average Loss: 3.878, avg. samples / sec: 1940.28
Iteration:   6680, Loss function: 3.740, Average Loss: 3.876, avg. samples / sec: 1947.03
Iteration:   6700, Loss function: 3.916, Average Loss: 3.874, avg. samples / sec: 1949.18
Iteration:   6720, Loss function: 3.739, Average Loss: 3.873, avg. samples / sec: 1941.52
Iteration:   6740, Loss function: 3.975, Average Loss: 3.871, avg. samples / sec: 1952.55
Iteration:   6760, Loss function: 3.709, Average Loss: 3.869, avg. samples / sec: 1940.43
Iteration:   6780, Loss function: 3.643, Average Loss: 3.867, avg. samples / sec: 1946.27
Iteration:   6800, Loss function: 4.023, Average Loss: 3.866, avg. samples / sec: 1943.29

:::MLPv0.5.0 ssd 1560275557.092828989 (train.py:553) train_epoch: 35
Iteration:   6820, Loss function: 3.613, Average Loss: 3.866, avg. samples / sec: 1946.97
Iteration:   6840, Loss function: 3.841, Average Loss: 3.865, avg. samples / sec: 1939.99
Iteration:   6860, Loss function: 3.691, Average Loss: 3.862, avg. samples / sec: 1949.81
Iteration:   6880, Loss function: 3.661, Average Loss: 3.861, avg. samples / sec: 1947.74
Iteration:   6900, Loss function: 3.461, Average Loss: 3.859, avg. samples / sec: 1948.63
Iteration:   6920, Loss function: 3.695, Average Loss: 3.857, avg. samples / sec: 1945.92
Iteration:   6940, Loss function: 4.031, Average Loss: 3.855, avg. samples / sec: 1946.35
Iteration:   6960, Loss function: 3.585, Average Loss: 3.852, avg. samples / sec: 1947.20
Iteration:   6980, Loss function: 3.768, Average Loss: 3.849, avg. samples / sec: 1951.17
Iteration:   7000, Loss function: 4.186, Average Loss: 3.847, avg. samples / sec: 1946.82

:::MLPv0.5.0 ssd 1560275617.674682379 (train.py:553) train_epoch: 36
Iteration:   7020, Loss function: 3.861, Average Loss: 3.846, avg. samples / sec: 1949.40
Iteration:   7040, Loss function: 3.719, Average Loss: 3.845, avg. samples / sec: 1943.99
Iteration:   7060, Loss function: 4.012, Average Loss: 3.842, avg. samples / sec: 1945.05
Iteration:   7080, Loss function: 3.913, Average Loss: 3.840, avg. samples / sec: 1946.52
Iteration:   7100, Loss function: 4.067, Average Loss: 3.840, avg. samples / sec: 1942.79
Iteration:   7120, Loss function: 3.592, Average Loss: 3.839, avg. samples / sec: 1948.72
Iteration:   7140, Loss function: 3.422, Average Loss: 3.837, avg. samples / sec: 1947.96
Iteration:   7160, Loss function: 3.809, Average Loss: 3.836, avg. samples / sec: 1948.58
Iteration:   7180, Loss function: 3.761, Average Loss: 3.836, avg. samples / sec: 1947.34

:::MLPv0.5.0 ssd 1560275678.584822178 (train.py:553) train_epoch: 37
Iteration:   7200, Loss function: 3.847, Average Loss: 3.835, avg. samples / sec: 1943.59
Iteration:   7220, Loss function: 3.524, Average Loss: 3.833, avg. samples / sec: 1945.39
Iteration:   7240, Loss function: 3.886, Average Loss: 3.832, avg. samples / sec: 1944.94
Iteration:   7260, Loss function: 3.804, Average Loss: 3.831, avg. samples / sec: 1951.23
Iteration:   7280, Loss function: 3.804, Average Loss: 3.831, avg. samples / sec: 1948.33
Iteration:   7300, Loss function: 3.647, Average Loss: 3.829, avg. samples / sec: 1938.01
Iteration:   7320, Loss function: 3.874, Average Loss: 3.827, avg. samples / sec: 1947.78
Iteration:   7340, Loss function: 3.525, Average Loss: 3.826, avg. samples / sec: 1945.25
Iteration:   7360, Loss function: 3.641, Average Loss: 3.826, avg. samples / sec: 1947.73
Iteration:   7380, Loss function: 3.719, Average Loss: 3.823, avg. samples / sec: 1941.57

:::MLPv0.5.0 ssd 1560275739.230701923 (train.py:553) train_epoch: 38
Iteration:   7400, Loss function: 3.852, Average Loss: 3.821, avg. samples / sec: 1940.39
Iteration:   7420, Loss function: 3.829, Average Loss: 3.821, avg. samples / sec: 1940.37
Iteration:   7440, Loss function: 3.563, Average Loss: 3.820, avg. samples / sec: 1943.50
Iteration:   7460, Loss function: 3.511, Average Loss: 3.818, avg. samples / sec: 1942.85
Iteration:   7480, Loss function: 3.717, Average Loss: 3.817, avg. samples / sec: 1942.80
Iteration:   7500, Loss function: 3.686, Average Loss: 3.815, avg. samples / sec: 1942.39
Iteration:   7520, Loss function: 3.701, Average Loss: 3.815, avg. samples / sec: 1942.15
Iteration:   7540, Loss function: 3.985, Average Loss: 3.813, avg. samples / sec: 1944.22
Iteration:   7560, Loss function: 3.808, Average Loss: 3.812, avg. samples / sec: 1945.28
Iteration:   7580, Loss function: 3.867, Average Loss: 3.812, avg. samples / sec: 1942.64

:::MLPv0.5.0 ssd 1560275800.245738029 (train.py:553) train_epoch: 39
Iteration:   7600, Loss function: 3.482, Average Loss: 3.810, avg. samples / sec: 1944.88
Iteration:   7620, Loss function: 3.754, Average Loss: 3.809, avg. samples / sec: 1942.75
Iteration:   7640, Loss function: 3.731, Average Loss: 3.807, avg. samples / sec: 1940.23
Iteration:   7660, Loss function: 3.500, Average Loss: 3.805, avg. samples / sec: 1943.71
Iteration:   7680, Loss function: 3.667, Average Loss: 3.804, avg. samples / sec: 1941.56
Iteration:   7700, Loss function: 3.847, Average Loss: 3.803, avg. samples / sec: 1947.20
Iteration:   7720, Loss function: 3.924, Average Loss: 3.802, avg. samples / sec: 1947.16
Iteration:   7740, Loss function: 3.758, Average Loss: 3.799, avg. samples / sec: 1945.88
Iteration:   7760, Loss function: 3.662, Average Loss: 3.796, avg. samples / sec: 1942.46
Iteration:   7780, Loss function: 3.498, Average Loss: 3.794, avg. samples / sec: 1945.70

:::MLPv0.5.0 ssd 1560275861.231321573 (train.py:553) train_epoch: 40
Iteration:   7800, Loss function: 3.596, Average Loss: 3.793, avg. samples / sec: 1944.45
Iteration:   7820, Loss function: 3.969, Average Loss: 3.791, avg. samples / sec: 1945.08
Iteration:   7840, Loss function: 3.635, Average Loss: 3.789, avg. samples / sec: 1944.04
Iteration:   7860, Loss function: 3.685, Average Loss: 3.788, avg. samples / sec: 1946.09
Iteration:   7880, Loss function: 3.845, Average Loss: 3.789, avg. samples / sec: 1942.38
Iteration:   7900, Loss function: 3.674, Average Loss: 3.787, avg. samples / sec: 1950.26
Iteration:   7920, Loss function: 3.683, Average Loss: 3.784, avg. samples / sec: 1947.25
Iteration:   7940, Loss function: 3.724, Average Loss: 3.782, avg. samples / sec: 1946.40
Iteration:   7960, Loss function: 4.050, Average Loss: 3.781, avg. samples / sec: 1951.07

:::MLPv0.5.0 ssd 1560275921.833433867 (train.py:553) train_epoch: 41
Iteration:   7980, Loss function: 3.808, Average Loss: 3.782, avg. samples / sec: 1946.51
Iteration:   8000, Loss function: 3.749, Average Loss: 3.781, avg. samples / sec: 1939.62
Iteration:   8020, Loss function: 3.755, Average Loss: 3.779, avg. samples / sec: 1941.81
Iteration:   8040, Loss function: 3.737, Average Loss: 3.779, avg. samples / sec: 1944.90
Iteration:   8060, Loss function: 3.499, Average Loss: 3.778, avg. samples / sec: 1942.06
Iteration:   8080, Loss function: 3.664, Average Loss: 3.777, avg. samples / sec: 1938.01
Iteration:   8100, Loss function: 3.777, Average Loss: 3.777, avg. samples / sec: 1941.38
Iteration:   8120, Loss function: 3.776, Average Loss: 3.776, avg. samples / sec: 1941.80
Iteration:   8140, Loss function: 3.636, Average Loss: 3.775, avg. samples / sec: 1947.13
Iteration:   8160, Loss function: 3.777, Average Loss: 3.773, avg. samples / sec: 1944.24

:::MLPv0.5.0 ssd 1560275982.869399548 (train.py:553) train_epoch: 42
Iteration:   8180, Loss function: 3.664, Average Loss: 3.771, avg. samples / sec: 1942.66
Iteration:   8200, Loss function: 3.805, Average Loss: 3.772, avg. samples / sec: 1944.53
Iteration:   8220, Loss function: 3.821, Average Loss: 3.770, avg. samples / sec: 1947.07
Iteration:   8240, Loss function: 3.694, Average Loss: 3.768, avg. samples / sec: 1945.42
Iteration:   8260, Loss function: 4.080, Average Loss: 3.766, avg. samples / sec: 1947.80
Iteration:   8280, Loss function: 3.799, Average Loss: 3.765, avg. samples / sec: 1932.98
Iteration:   8300, Loss function: 3.776, Average Loss: 3.765, avg. samples / sec: 1947.71
Iteration:   8320, Loss function: 3.604, Average Loss: 3.764, avg. samples / sec: 1948.86
Iteration:   8340, Loss function: 3.838, Average Loss: 3.762, avg. samples / sec: 1944.04
Iteration:   8360, Loss function: 3.573, Average Loss: 3.761, avg. samples / sec: 1942.48

:::MLPv0.5.0 ssd 1560276043.530421257 (train.py:553) train_epoch: 43
Iteration:   8380, Loss function: 3.923, Average Loss: 3.761, avg. samples / sec: 1947.01
Iteration:   8400, Loss function: 3.659, Average Loss: 3.760, avg. samples / sec: 1949.18
Iteration:   8420, Loss function: 3.876, Average Loss: 3.759, avg. samples / sec: 1951.69
lr decay step #1

:::MLPv0.5.0 ssd 1560276060.705424786 (train.py:578) opt_learning_rate: 0.0048000000000000004





:::MLPv0.5.0 ssd 1560276061.009387255 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1560276061.010322094 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1560276061.011115789 (train.py:220) eval_start: 43
Parsing batch: 0/12Parsing batch: 0/12Parsing batch: 0/12Parsing batch: 0/12Parsing batch: 1/12Parsing batch: 1/12Parsing batch: 1/12Parsing batch: 1/12Parsing batch: 2/12Parsing batch: 2/12Parsing batch: 2/12Parsing batch: 2/12Parsing batch: 3/12Parsing batch: 3/12Parsing batch: 3/12Parsing batch: 3/12Parsing batch: 4/12Parsing batch: 4/12Parsing batch: 4/12Parsing batch: 4/12Parsing batch: 5/12Parsing batch: 5/12Parsing batch: 5/12Parsing batch: 5/12Parsing batch: 6/12Parsing batch: 6/12Parsing batch: 6/12Parsing batch: 6/12Parsing batch: 7/12Parsing batch: 7/12Parsing batch: 7/12Parsing batch: 7/12Parsing batch: 8/12Parsing batch: 8/12Parsing batch: 8/12Parsing batch: 8/12Parsing batch: 9/12Parsing batch: 9/12Parsing batch: 9/12Parsing batch: 9/12Parsing batch: 10/12Parsing batch: 10/12Parsing batch: 10/12Parsing batch: 10/12Parsing batch: 11/12Parsing batch: 11/12Parsing batch: 11/12Parsing batch: 11/12Predicting Ended, total time: 67.91 s
Loading and preparing results...
Converting ndarray to lists...
(346830, 7)
0/346830
Loading and preparing results...
Converting ndarray to lists...
(346830, 7)
0/346830
Loading and preparing results...
Converting ndarray to lists...
(346830, 7)
0/346830
Loading and preparing results...
Converting ndarray to lists...
(346830, 7)
0/346830
DONE (t=2.28s)
creating index...
DONE (t=2.41s)
creating index...
index created!
DONE (t=2.43s)
creating index...
DONE (t=2.49s)
creating index...
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=4.04s).
Accumulating evaluation results...
DONE (t=1.64s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.306
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.042
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.173
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.257
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.254
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.267
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.075
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.287
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.413
Current AP: 0.15998 AP goal: 0.21200

:::MLPv0.5.0 ssd 1560276137.245995522 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1560276137.246632099 (train.py:333) eval_accuracy: {"epoch": 43, "value": 0.1599778783223212}

:::MLPv0.5.0 ssd 1560276137.247152567 (train.py:336) eval_iteration_accuracy: {"epoch": 43, "value": 0.1599778783223212}

:::MLPv0.5.0 ssd 1560276137.247659683 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1560276137.248256683 (train.py:338) eval_stop: 43
Iteration:   8440, Loss function: 3.348, Average Loss: 3.755, avg. samples / sec: 146.71
Iteration:   8460, Loss function: 3.501, Average Loss: 3.748, avg. samples / sec: 1961.92
Iteration:   8480, Loss function: 3.258, Average Loss: 3.739, avg. samples / sec: 1961.60
Iteration:   8500, Loss function: 3.014, Average Loss: 3.730, avg. samples / sec: 1948.66
Iteration:   8520, Loss function: 3.046, Average Loss: 3.720, avg. samples / sec: 1948.90
Iteration:   8540, Loss function: 3.239, Average Loss: 3.711, avg. samples / sec: 1951.64
Iteration:   8560, Loss function: 3.116, Average Loss: 3.700, avg. samples / sec: 1956.33

:::MLPv0.5.0 ssd 1560276180.892713308 (train.py:553) train_epoch: 44
Iteration:   8580, Loss function: 3.242, Average Loss: 3.689, avg. samples / sec: 1959.49
Iteration:   8600, Loss function: 3.157, Average Loss: 3.678, avg. samples / sec: 1959.88
Iteration:   8620, Loss function: 3.004, Average Loss: 3.667, avg. samples / sec: 1954.42
Iteration:   8640, Loss function: 3.331, Average Loss: 3.658, avg. samples / sec: 1949.65
Iteration:   8660, Loss function: 3.358, Average Loss: 3.650, avg. samples / sec: 1949.25
Iteration:   8680, Loss function: 2.987, Average Loss: 3.641, avg. samples / sec: 1950.86
Iteration:   8700, Loss function: 3.213, Average Loss: 3.630, avg. samples / sec: 1946.92
Iteration:   8720, Loss function: 3.375, Average Loss: 3.620, avg. samples / sec: 1937.61
Iteration:   8740, Loss function: 3.103, Average Loss: 3.611, avg. samples / sec: 1939.79

:::MLPv0.5.0 ssd 1560276241.407684326 (train.py:553) train_epoch: 45
Iteration:   8760, Loss function: 3.168, Average Loss: 3.601, avg. samples / sec: 1944.93
Iteration:   8780, Loss function: 3.068, Average Loss: 3.591, avg. samples / sec: 1942.62
Iteration:   8800, Loss function: 3.073, Average Loss: 3.581, avg. samples / sec: 1946.42
Iteration:   8820, Loss function: 3.110, Average Loss: 3.572, avg. samples / sec: 1950.98
Iteration:   8840, Loss function: 3.204, Average Loss: 3.564, avg. samples / sec: 1942.13
Iteration:   8860, Loss function: 3.290, Average Loss: 3.555, avg. samples / sec: 1937.27
Iteration:   8880, Loss function: 3.003, Average Loss: 3.547, avg. samples / sec: 1949.85
Iteration:   8900, Loss function: 3.092, Average Loss: 3.539, avg. samples / sec: 1947.94
Iteration:   8920, Loss function: 3.200, Average Loss: 3.531, avg. samples / sec: 1939.44
Iteration:   8940, Loss function: 2.926, Average Loss: 3.522, avg. samples / sec: 1937.36

:::MLPv0.5.0 ssd 1560276302.399715185 (train.py:553) train_epoch: 46
Iteration:   8960, Loss function: 3.120, Average Loss: 3.512, avg. samples / sec: 1940.51
Iteration:   8980, Loss function: 3.001, Average Loss: 3.504, avg. samples / sec: 1946.90
Iteration:   9000, Loss function: 3.071, Average Loss: 3.495, avg. samples / sec: 1934.07
Iteration:   9020, Loss function: 3.138, Average Loss: 3.486, avg. samples / sec: 1945.48
Iteration:   9040, Loss function: 2.976, Average Loss: 3.478, avg. samples / sec: 1951.06
Iteration:   9060, Loss function: 3.053, Average Loss: 3.470, avg. samples / sec: 1943.52
Iteration:   9080, Loss function: 3.204, Average Loss: 3.464, avg. samples / sec: 1943.76
Iteration:   9100, Loss function: 3.132, Average Loss: 3.456, avg. samples / sec: 1943.79
Iteration:   9120, Loss function: 3.051, Average Loss: 3.448, avg. samples / sec: 1939.46
Iteration:   9140, Loss function: 2.962, Average Loss: 3.440, avg. samples / sec: 1939.19

:::MLPv0.5.0 ssd 1560276363.110841036 (train.py:553) train_epoch: 47
Iteration:   9160, Loss function: 3.042, Average Loss: 3.432, avg. samples / sec: 1941.93
Iteration:   9180, Loss function: 2.873, Average Loss: 3.423, avg. samples / sec: 1946.19
Iteration:   9200, Loss function: 2.945, Average Loss: 3.415, avg. samples / sec: 1942.55
Iteration:   9220, Loss function: 3.054, Average Loss: 3.408, avg. samples / sec: 1942.42
Iteration:   9240, Loss function: 3.090, Average Loss: 3.401, avg. samples / sec: 1939.21
Iteration:   9260, Loss function: 2.962, Average Loss: 3.394, avg. samples / sec: 1938.12
Iteration:   9280, Loss function: 3.143, Average Loss: 3.387, avg. samples / sec: 1945.02
Iteration:   9300, Loss function: 2.967, Average Loss: 3.379, avg. samples / sec: 1943.81
Iteration:   9320, Loss function: 2.996, Average Loss: 3.371, avg. samples / sec: 1937.95

:::MLPv0.5.0 ssd 1560276424.179838419 (train.py:553) train_epoch: 48
Iteration:   9340, Loss function: 3.196, Average Loss: 3.364, avg. samples / sec: 1938.02
Iteration:   9360, Loss function: 2.974, Average Loss: 3.356, avg. samples / sec: 1942.02
Iteration:   9380, Loss function: 2.974, Average Loss: 3.349, avg. samples / sec: 1941.94
Iteration:   9400, Loss function: 3.056, Average Loss: 3.342, avg. samples / sec: 1944.00
Iteration:   9420, Loss function: 3.125, Average Loss: 3.336, avg. samples / sec: 1944.57
Iteration:   9440, Loss function: 3.118, Average Loss: 3.331, avg. samples / sec: 1949.95
Iteration:   9460, Loss function: 2.907, Average Loss: 3.325, avg. samples / sec: 1951.74





:::MLPv0.5.0 ssd 1560276466.363386154 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1560276466.364022255 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1560276466.364522457 (train.py:220) eval_start: 48
Parsing batch: 0/12Parsing batch: 0/12Parsing batch: 0/12Parsing batch: 0/12Parsing batch: 1/12Parsing batch: 1/12Parsing batch: 1/12Parsing batch: 1/12Parsing batch: 2/12Parsing batch: 2/12Parsing batch: 2/12Parsing batch: 2/12Parsing batch: 3/12Parsing batch: 3/12Parsing batch: 3/12Parsing batch: 3/12Parsing batch: 4/12Parsing batch: 4/12Parsing batch: 4/12Parsing batch: 4/12Parsing batch: 5/12Parsing batch: 5/12Parsing batch: 5/12Parsing batch: 5/12Parsing batch: 6/12Parsing batch: 6/12Parsing batch: 6/12Parsing batch: 6/12Parsing batch: 7/12Parsing batch: 7/12Parsing batch: 7/12Parsing batch: 7/12Parsing batch: 8/12Parsing batch: 8/12Parsing batch: 8/12Parsing batch: 8/12Parsing batch: 9/12Parsing batch: 9/12Parsing batch: 9/12Parsing batch: 9/12Parsing batch: 10/12Parsing batch: 10/12Parsing batch: 10/12Parsing batch: 10/12Parsing batch: 11/12Parsing batch: 11/12Parsing batch: 11/12Parsing batch: 11/12Predicting Ended, total time: 62.42 s
Loading and preparing results...
Converting ndarray to lists...
(303515, 7)
0/303515
Loading and preparing results...
Converting ndarray to lists...
(303515, 7)
0/303515
Loading and preparing results...
Converting ndarray to lists...
(303515, 7)
0/303515
Loading and preparing results...
Converting ndarray to lists...
(303515, 7)
0/303515
DONE (t=1.76s)
creating index...
DONE (t=1.89s)
creating index...
index created!
DONE (t=1.93s)
creating index...
DONE (t=1.99s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=3.63s).
Accumulating evaluation results...
DONE (t=1.24s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.377
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.351
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.213
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.311
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.326
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.101
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.509
Current AP: 0.21778 AP goal: 0.21200

:::MLPv0.5.0 ssd 1560276535.991231441 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1560276535.992122889 (train.py:333) eval_accuracy: {"epoch": 48, "value": 0.21778021397101513}

:::MLPv0.5.0 ssd 1560276535.992647886 (train.py:336) eval_iteration_accuracy: {"epoch": 48, "value": 0.21778021397101513}

:::MLPv0.5.0 ssd 1560276535.993154287 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1560276535.993930340 (train.py:338) eval_stop: 48

:::MLPv0.5.0 ssd 1560276537.100983620 (train.py:706) run_stop: {"success": true}

:::MLPv0.5.0 ssd 1560276537.101498127 (train.py:707) run_final
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-06-11 06:09:01 PM
RESULT,OBJECT_DETECTION,,3280,nvidia,2019-06-11 05:14:21 PM
